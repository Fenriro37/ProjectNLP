{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgHMqcneDn1J"
      },
      "source": [
        "- We loose structure of dialogue using the tokenizer in preprocess_data\n",
        "\n",
        "- Emotions encoded using MultiLabelBinarizer doesn't tell us\n",
        "  anymore how many times a single emotion is present in the dialogue\n",
        "  and where it is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2024-01-12T18:25:52.423261Z",
          "iopub.status.busy": "2024-01-12T18:25:52.422488Z",
          "iopub.status.idle": "2024-01-12T18:29:06.936875Z",
          "shell.execute_reply": "2024-01-12T18:29:06.935874Z",
          "shell.execute_reply.started": "2024-01-12T18:25:52.423224Z"
        },
        "id": "vc-YdlGVyh9_",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "5f5d1168-047a-4b38-fd46-a1499a8fdfde",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu116\n",
            "Collecting torch==1.13.0+cu116\n",
            "  Downloading https://download.pytorch.org/whl/cu116/torch-1.13.0%2Bcu116-cp310-cp310-linux_x86_64.whl (1983.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 GB\u001b[0m \u001b[31m348.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.13.0+cu116) (4.5.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.0+cu116 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.0+cu116 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.0+cu116 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.0+cu116 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.13.0+cu116\n",
            "Collecting transformers==4.30.0\n",
            "  Downloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.30.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.30.0) (2023.11.17)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed tokenizers-0.13.3 transformers-4.30.0\n",
            "Collecting datasets==2.13.2\n",
            "  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (10.0.1)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets==2.13.2)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.13.2)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.13.2) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.13.2) (4.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.2) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.2) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.2) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.13.2) (2023.11.17)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.13.2) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.13.2) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.13.2 dill-0.3.6 multiprocess-0.70.14\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.13.0+cu116)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.26.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (23.2)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (10.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==1.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
        "!pip install transformers==4.30.0\n",
        "!pip install datasets==2.13.2\n",
        "!pip install accelerate -U\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:06.939068Z",
          "iopub.status.busy": "2024-01-12T18:29:06.938752Z",
          "iopub.status.idle": "2024-01-12T18:29:27.003689Z",
          "shell.execute_reply": "2024-01-12T18:29:27.002668Z",
          "shell.execute_reply.started": "2024-01-12T18:29:06.939042Z"
        },
        "id": "pD7fWLEPyh-B",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# system packages\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import urllib\n",
        "import tarfile\n",
        "import sys\n",
        "import os\n",
        "# data and numerical management packages\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# useful during debugging (progress bars)\n",
        "from tqdm import tqdm\n",
        "from transformers import set_seed\n",
        "\n",
        "seed = 852\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:27.005734Z",
          "iopub.status.busy": "2024-01-12T18:29:27.004994Z",
          "iopub.status.idle": "2024-01-12T18:29:27.161176Z",
          "shell.execute_reply": "2024-01-12T18:29:27.160394Z",
          "shell.execute_reply.started": "2024-01-12T18:29:27.005698Z"
        },
        "id": "6Vnl6q1Qyh-C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "dataset_folder = Path.cwd().joinpath(\"Datasets/MELD_train_efr.json\")\n",
        "#dataset_path = dataset_folder.joinpath('/MELD_train_efr.json')\n",
        "df = pd.read_json(dataset_folder)\n",
        "#df['triggers'] = df['triggers'].fillna(value=0, inplace=False)#.replace('None', 0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:27.164488Z",
          "iopub.status.busy": "2024-01-12T18:29:27.164068Z",
          "iopub.status.idle": "2024-01-12T18:29:27.353387Z",
          "shell.execute_reply": "2024-01-12T18:29:27.352697Z",
          "shell.execute_reply.started": "2024-01-12T18:29:27.164451Z"
        },
        "id": "PK4MQkLvyh-C",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "triggers = df['triggers']\n",
        "for row in range(triggers.shape[0]):\n",
        "    for trigger in range(len(triggers[row])):\n",
        "        if triggers[row][trigger] == None:\n",
        "            triggers[row][trigger] = 0.0\n",
        "\n",
        "df['triggers'] = triggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:27.354645Z",
          "iopub.status.busy": "2024-01-12T18:29:27.354368Z",
          "iopub.status.idle": "2024-01-12T18:29:27.374027Z",
          "shell.execute_reply": "2024-01-12T18:29:27.373203Z",
          "shell.execute_reply.started": "2024-01-12T18:29:27.354621Z"
        },
        "id": "QFfykazN2xDw",
        "outputId": "8068426f-325b-424e-9a73-a64568df0dcb",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neutral', 'surprise', 'fear', 'sadness', 'joy', 'disgust',\n",
              "       'anger'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "emotions = df['emotions'].explode().unique()\n",
        "emotions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:27.375319Z",
          "iopub.status.busy": "2024-01-12T18:29:27.375078Z",
          "iopub.status.idle": "2024-01-12T18:29:27.390235Z",
          "shell.execute_reply": "2024-01-12T18:29:27.389245Z",
          "shell.execute_reply.started": "2024-01-12T18:29:27.375298Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p10v1luzDn1M",
        "outputId": "ca39f0ca-0d34-43f4-d2c8-b6ee47a2c046"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.0, 1.0], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "triggers = df['triggers'].explode().unique()\n",
        "triggers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2024-01-12T18:29:27.392361Z",
          "iopub.status.busy": "2024-01-12T18:29:27.391490Z",
          "iopub.status.idle": "2024-01-12T18:29:27.412862Z",
          "shell.execute_reply": "2024-01-12T18:29:27.411917Z",
          "shell.execute_reply.started": "2024-01-12T18:29:27.392329Z"
        },
        "id": "q39q6ZPh-tCy",
        "outputId": "5ffa01bb-161c-40a1-8d79-b9889c5ee1eb",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24, 219)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dialogues = df['utterances']\n",
        "#print(sentences)\n",
        "max_len_utterances = 0\n",
        "index = 0\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "  if len(dialogue) > max_len_utterances:\n",
        "    max_len_utterances = len(dialogue)\n",
        "    index = idx\n",
        "max_len_utterances,index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = df['utterances']\n",
        "#print(sentences)\n",
        "max_len_sentence = 0\n",
        "index = 0\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "  for idx_sentence, utterance in enumerate(dialogue):\n",
        "    if len(utterance.split()) > max_len_sentence:\n",
        "      max_len_sentence = len(utterance.split())\n",
        "      index = idx,idx_sentence\n",
        "max_len_sentence,index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwdTYBlTMvli",
        "outputId": "208cb9a2-7314-4bcf-8237-a3ec630ae754"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(69, (1675, 11))"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:32:51.180932Z",
          "iopub.status.busy": "2024-01-12T18:32:51.180195Z",
          "iopub.status.idle": "2024-01-12T18:32:57.227091Z",
          "shell.execute_reply": "2024-01-12T18:32:57.226294Z",
          "shell.execute_reply.started": "2024-01-12T18:32:51.180896Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R-6uNvEDn1O",
        "outputId": "d17f803c-0936-41a6-da8b-6a2ffbae2e2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding\n",
        "model_card = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_card)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_card,\n",
        "                                                           num_labels=24,#len(emotions)+1, # +1 padding\n",
        "                                                           problem_type=\"multi_label_classification\")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bP0X_ZEvFSp"
      },
      "source": [
        "### Be careful for the token used during padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:30:39.880460Z",
          "iopub.status.busy": "2024-01-12T18:30:39.879732Z",
          "iopub.status.idle": "2024-01-12T18:30:39.889776Z",
          "shell.execute_reply": "2024-01-12T18:30:39.888865Z",
          "shell.execute_reply.started": "2024-01-12T18:30:39.880426Z"
        },
        "trusted": true,
        "id": "HHLo6FwYDn1M"
      },
      "outputs": [],
      "source": [
        "padded_dialogues = [seq + ['[PAD]'] * (max_len_utterances - len(seq)) for seq in dialogues]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:30:45.278400Z",
          "iopub.status.busy": "2024-01-12T18:30:45.278059Z",
          "iopub.status.idle": "2024-01-12T18:30:45.517596Z",
          "shell.execute_reply": "2024-01-12T18:30:45.516517Z",
          "shell.execute_reply.started": "2024-01-12T18:30:45.278375Z"
        },
        "trusted": true,
        "id": "FsgpSVdgDn1M"
      },
      "outputs": [],
      "source": [
        "padded_emotions = [seq + ['[PAD]'] * (max_len_utterances - len(seq)) for seq in df['emotions']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:31:01.496584Z",
          "iopub.status.busy": "2024-01-12T18:31:01.496212Z",
          "iopub.status.idle": "2024-01-12T18:31:01.506197Z",
          "shell.execute_reply": "2024-01-12T18:31:01.505122Z",
          "shell.execute_reply.started": "2024-01-12T18:31:01.496553Z"
        },
        "id": "-PH9rccxqc6g",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "triggers = df['triggers']\n",
        "padded_triggers = [seq + [0] * (max_len_utterances - len(seq)) for seq in triggers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:31:04.443590Z",
          "iopub.status.busy": "2024-01-12T18:31:04.443208Z",
          "iopub.status.idle": "2024-01-12T18:31:04.493733Z",
          "shell.execute_reply": "2024-01-12T18:31:04.492236Z",
          "shell.execute_reply.started": "2024-01-12T18:31:04.443561Z"
        },
        "id": "Cg7CppRVvg0-",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7dd7fc-a1c5-4943-f27d-5de517426e30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "also I was the point person on my company's transition from the KL-5 to GR-6 system.\n",
            "[tensor([  101,  2036,  1045,  2001,  1996,  2391,  2711,  2006,  2026,  2194,\n",
            "         1005,  1055,  6653,  2013,  1996,  1047,  2140,  1011,  1019,  2000,\n",
            "        24665,  1011,  1020,  2291,  1012,   102])]\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "tokenized_input_ids = []\n",
        "tokenized_attention_mask = []\n",
        "tokenized_token_type_ids = []\n",
        "i = 0\n",
        "for utterance_list in padded_dialogues:\n",
        "  utterance_input_ids = []\n",
        "  utterance_attention_mask = []\n",
        "  utterance_token_type_ids = []\n",
        "  for utterance in utterance_list:\n",
        "    tokenized_texts = tokenizer(utterance,return_tensors='pt')# padding='max_length', max_length=max_len, )\n",
        "    utterance_input_ids.extend(tokenized_texts.input_ids)\n",
        "    utterance_attention_mask.extend(tokenized_texts.attention_mask)\n",
        "    utterance_token_type_ids.extend(tokenized_texts.token_type_ids)\n",
        "\"\"\"    if i == 0:\n",
        "        print(utterance)\n",
        "        print(utterance_input_ids)\n",
        "        print(tokenized_input_ids)\n",
        "        i+=1\"\"\"\n",
        "  tokenized_input_ids.append(utterance_input_ids)\n",
        "  tokenized_attention_mask.append(utterance_attention_mask)\n",
        "  tokenized_token_type_ids.append(utterance_token_type_ids)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.077108Z",
          "iopub.status.idle": "2024-01-12T18:29:29.077562Z",
          "shell.execute_reply": "2024-01-12T18:29:29.077331Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.077311Z"
        },
        "id": "va6wzu7gx9nw",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "emotions_input_ids = []\n",
        "emotions_attention_mask = []\n",
        "emotions_token_type_ids = []\n",
        "\n",
        "for utterance_list in padded_emotions:\n",
        "  emotion_input_ids = []\n",
        "  emotion_attention_mask = []\n",
        "  emotion_token_type_ids = []\n",
        "  for utterance in utterance_list:\n",
        "    tokenized_texts = tokenizer(utterance,return_tensors='pt')# padding='max_length', max_length=max_len, )\n",
        "    emotion_input_ids.append(tokenized_texts.input_ids)\n",
        "    emotion_attention_mask.append(tokenized_texts.attention_mask)\n",
        "    emotion_token_type_ids.append(tokenized_texts.token_type_ids)\n",
        "\n",
        "  emotions_input_ids.append(emotion_input_ids)\n",
        "  emotions_attention_mask.append(emotion_attention_mask)\n",
        "  emotions_token_type_ids.append(emotion_token_type_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.078716Z",
          "iopub.status.idle": "2024-01-12T18:29:29.079188Z",
          "shell.execute_reply": "2024-01-12T18:29:29.078961Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.078938Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDNbg7j_Dn1N",
        "outputId": "67731fa4-e003-4115-d165-a0c94cb7f45c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [neutral, neutral, neutral, neutral, surprise]\n",
              "1    [neutral, neutral, neutral, neutral, surprise,...\n",
              "2    [neutral, neutral, neutral, neutral, surprise,...\n",
              "3    [neutral, neutral, neutral, neutral, surprise,...\n",
              "4                  [surprise, sadness, surprise, fear]\n",
              "Name: emotions, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "df['emotions'][:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.083480Z",
          "iopub.status.idle": "2024-01-12T18:29:29.083825Z",
          "shell.execute_reply": "2024-01-12T18:29:29.083646Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.083631Z"
        },
        "id": "xgporu7myOr3",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "pad = torch.from_numpy(np.array([0]))\n",
        "padded_utterances_input_ids = [seq + [pad] * (max_len - len(seq)) for seq in tokenized_input_ids]\n",
        "padded_utterances_attention_mask = [seq + [torch.from_numpy(np.array([1]))] * (max_len - len(seq)) for seq in tokenized_attention_mask]\n",
        "padded_utterances_token_type_ids = [seq + [pad] * (max_len - len(seq)) for seq in tokenized_token_type_ids]\n",
        "pad_as_1 = torch.from_numpy(np.array([1]))\n",
        "padded_emotions_input_ids = [seq + [pad] * (max_len - len(seq)) for seq in emotions_input_ids]\n",
        "padded_emotions_attention_mask = [seq + [pad_as_1] * (max_len - len(seq)) for seq in emotions_attention_mask]\n",
        "padded_emotions_token_type_ids = [seq + [pad] * (max_len - len(seq)) for seq in emotions_token_type_ids]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.084890Z",
          "iopub.status.idle": "2024-01-12T18:29:29.085217Z",
          "shell.execute_reply": "2024-01-12T18:29:29.085070Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.085054Z"
        },
        "trusted": true,
        "id": "EZDF8laGDn1N"
      },
      "outputs": [],
      "source": [
        "emotion_mapping = {emotion: idx+1 for idx, emotion in enumerate(emotions)}\n",
        "emotions_numerical_format = [[emotion_mapping[emotion] for emotion in dialogue] for dialogue in df['emotions']]\n",
        "padded_emotions = [seq + [0] * (max_len_utterances - len(seq)) for seq in emotions_numerical_format]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_np = np.array(padded_emotions, dtype=int)\n",
        "labels_np[0.8*4000: 0.8*4000+400]"
      ],
      "metadata": {
        "id": "XD6cQBTfntQE"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:31:29.907488Z",
          "iopub.status.busy": "2024-01-12T18:31:29.907098Z",
          "iopub.status.idle": "2024-01-12T18:31:30.467067Z",
          "shell.execute_reply": "2024-01-12T18:31:30.466119Z",
          "shell.execute_reply.started": "2024-01-12T18:31:29.907457Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IalqGDZDn1N",
        "outputId": "2703415a-0a0d-4714-a47d-154973a47b22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3200, 4), (400, 4), (400, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ],
      "source": [
        "data = {\n",
        "    'episode' : df['episode'],\n",
        "    'labels' : labels_np.tolist(),#emotions_input_ids, #tokenized emotions\n",
        "    'emotions' : padded_emotions,#emotions_input_ids, #tokenized emotions\n",
        "    'utterances': padded_dialogues,\n",
        "}\n",
        "df_tokenized = pd.DataFrame(data)\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_data, temp_data = train_test_split(df_tokenized, train_size=0.8, shuffle=False)\n",
        "val_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\n",
        "train_data.shape, val_data.shape,test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.8*4000: 0.8*4000+400"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjRLRa0Jowss",
        "outputId": "8ef8df6b-9e85-4946-929f-527ec60a6df1"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3200.0"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:31:32.620514Z",
          "iopub.status.busy": "2024-01-12T18:31:32.620181Z",
          "iopub.status.idle": "2024-01-12T18:31:33.260777Z",
          "shell.execute_reply": "2024-01-12T18:31:33.259836Z",
          "shell.execute_reply.started": "2024-01-12T18:31:32.620488Z"
        },
        "trusted": true,
        "id": "0t3b7RYBDn1O"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "training_hf = Dataset.from_pandas(train_data)\n",
        "validation_hf = Dataset.from_pandas(val_data)\n",
        "test_hf = Dataset.from_pandas(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['utterances']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWJqaEVXZEzK",
        "outputId": "74da76b2-0b64-4067-e448-7a919bce6fa2"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       [also I was the point person on my company's t...\n",
              "1       [also I was the point person on my company's t...\n",
              "2       [also I was the point person on my company's t...\n",
              "3       [also I was the point person on my company's t...\n",
              "4       [But then who? The waitress I went out with la...\n",
              "                              ...                        \n",
              "3195    [I can't believe this. Do you think that your ...\n",
              "3196    [I can't believe this. Do you think that your ...\n",
              "3197    [Dr. Geller, there's a seat over here., Thank ...\n",
              "3198    [Dr. Geller, there's a seat over here., Thank ...\n",
              "3199    [Dr. Geller, there's a seat over here., Thank ...\n",
              "Name: utterances, Length: 3200, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-01-12T18:33:25.558075Z",
          "iopub.status.busy": "2024-01-12T18:33:25.557724Z",
          "iopub.status.idle": "2024-01-12T18:33:47.730246Z",
          "shell.execute_reply": "2024-01-12T18:33:47.729333Z",
          "shell.execute_reply.started": "2024-01-12T18:33:25.558047Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "4NXHHI1fDn1O",
        "outputId": "1b50f8e1-b9c5-4f6b-a9b2-7c1f99e81021"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'training = training_hf.map(preprocess_text, batched=True)\\nvalidation = validation_hf.map(preprocess_text, batched=True)\\ntest = test_hf.map(preprocess_text, batched=True)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 239
        }
      ],
      "source": [
        "def preprocess_text(texts):\n",
        "    tokenized_inputs = tokenizer([\" \".join(utterance) for utterance in texts['utterances']],padding='max_length', truncation=True, max_length=max_len_sentence, return_tensors='pt')\n",
        "\n",
        "    labels_tensor = torch.tensor(labels_np[3200:3600,:], dtype=torch.float)\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': labels_tensor\n",
        "    })\n",
        "\"\"\"def preprocess_data(data, tokenizer, mlb):\n",
        "    # Tokenize the utterances\n",
        "    tokenized_inputs = tokenizer(\n",
        "        [\" \".join(utterance) for utterance in data['utterances']],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128, # we might wanna change this to max_len !\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Encode the emotions\n",
        "    encoded_labels = mlb.transform(data['emotions'])\n",
        "    # Convert labels to tensors\n",
        "    labels_tensor = torch.tensor(encoded_labels, dtype=torch.float)\n",
        "    # Ensure the encoded labels are the correct shape (this step is crucial to avoid the batch size mismatch)\n",
        "    if labels_tensor.shape[0] != tokenized_inputs['input_ids'].shape[0]:\n",
        "        raise ValueError(f\"Number of examples {tokenized_inputs['input_ids'].shape[0]} does not match number of labels {labels_tensor.shape[0]}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': labels_tensor\n",
        "    })\"\"\"\n",
        "\n",
        "# Now call preprocess_data to process your train and validation data\n",
        "#train_dataset = preprocess_text(train_data)\n",
        "val_dataset = preprocess_text(val_data)\n",
        "\n",
        "\"\"\"training = training_hf.map(preprocess_text, batched=True)\n",
        "validation = validation_hf.map(preprocess_text, batched=True)\n",
        "test = test_hf.map(preprocess_text, batched=True)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(val_dataset['input_ids'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJ9495yEp1TE",
        "outputId": "14036b85-9713-4b82-a43a-34b2d305403f"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2852, 1012, 21500, 3917, 1010, 2045, 1005, 1055, 1037, 2835, 2058, 2182, 1012, 4067, 2017, 1010, 2852, 1012, 8109, 1010, 2021, 1045, 1005, 1049, 2383, 2026, 6265, 2012, 2023, 2795, 1010, 2182, 1999, 1996, 2690, 1012, 1045, 1005, 1049, 2383, 6265, 2157, 2182, 1010, 2007, 2026, 2204, 2767, 9558, 1010, 2065, 2002, 1005, 2222, 4133, 2007, 2033, 1012, 1045, 2097, 4133, 2007, 2017, 2852, 1012, 21500, 3917, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.095177Z",
          "iopub.status.idle": "2024-01-12T18:29:29.095520Z",
          "shell.execute_reply": "2024-01-12T18:29:29.095374Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.095358Z"
        },
        "trusted": true,
        "id": "REXJ9ahkDn1O"
      },
      "outputs": [],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if 'embeddings' in name: # Layer names not containing 'classifier' will be frozen\n",
        "        param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.098643Z",
          "iopub.status.idle": "2024-01-12T18:29:29.099012Z",
          "shell.execute_reply": "2024-01-12T18:29:29.098866Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.098850Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "CcSpQVBPDn1O",
        "outputId": "0e88d27f-efa8-4975-e97f-801579b982f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:13, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.441000</td>\n",
              "      <td>0.264807</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=50, training_loss=0.440987548828125, metrics={'train_runtime': 13.7165, 'train_samples_per_second': 29.162, 'train_steps_per_second': 3.645, 'total_flos': 14186131948800.0, 'train_loss': 0.440987548828125, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"test_dir\",                 # where to save model\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=8,         # accelerate defines distributed training\n",
        "    per_device_eval_batch_size=50,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.1,\n",
        "    evaluation_strategy=\"epoch\",           # when to report evaluation metrics/losses\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",                 # when to save checkpoint\n",
        "    load_best_model_at_end=True,\n",
        "    report_to='none'                       # disabling wandb (default)\n",
        ")\n",
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=val_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    #compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction_info = trainer.predict(val_dataset)\n",
        "#metrics = compute_metrics(test_prediction_info)\n",
        "#print(test_prediction_info)\n",
        "test_prediction_info.predictions[0],test_prediction_info.label_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "YOTBKyU1s-i1",
        "outputId": "46985d03-82f8-48f5-db0a-bfc50e32eec1"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 0.15827195,  0.44049922,  0.5801743 ,  1.365504  ,  0.05035927,\n",
              "         1.0560385 ,  0.3032352 , -0.23196907,  0.6998879 ,  0.2703544 ,\n",
              "        -0.24004805,  0.4625907 , -0.6529549 ,  0.52764946, -0.5102945 ,\n",
              "        -0.36305633, -0.5296073 , -0.20032535,  0.3740353 , -0.26822382,\n",
              "        -0.4213328 , -0.44511944,  0.06515053, -1.1275369 ], dtype=float32),\n",
              " array([1., 1., 1., 1., 1., 4., 3., 1., 5., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_data['emotions'][3200]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0chv6R9ctHFl",
        "outputId": "7c3ff7af-61fa-48bd-b727-72d8c885e549"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 1, 1, 1, 1, 4, 3, 1, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ9XJj7DqQCe"
      },
      "source": [
        "### Nalin's scribbles below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.101779Z",
          "iopub.status.idle": "2024-01-12T18:29:29.102100Z",
          "shell.execute_reply": "2024-01-12T18:29:29.101953Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.101937Z"
        },
        "id": "006vgUiREp2_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # Apply sigmoid to logits and threshold to get multi-label predictions\n",
        "    sigmoid_logits = 1 / (1 + np.exp(-logits))  # Sigmoid function\n",
        "    threshold = 0.5\n",
        "    predictions = (sigmoid_logits > threshold).astype(int)\n",
        "\n",
        "    # Calculate F1 score for each individual label/class\n",
        "    f1_scores = [f1_score(labels[:, i], predictions[:, i], average='binary') for i in range(num_labels)]\n",
        "\n",
        "    # Sequence F1: compute the F1-score for each dialogue and report the average score\n",
        "    sequence_f1 = np.mean(f1_scores)\n",
        "\n",
        "    # Unrolled Sequence F1: flatten all utterances and compute the F1-score\n",
        "    unrolled_labels = labels.flatten()\n",
        "    unrolled_predictions = predictions.flatten()\n",
        "    unrolled_sequence_f1 = f1_score(unrolled_labels, unrolled_predictions, average='binary')\n",
        "\n",
        "    return {\n",
        "        'sequence_f1': sequence_f1,\n",
        "        'unrolled_sequence_f1': unrolled_sequence_f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.103311Z",
          "iopub.status.idle": "2024-01-12T18:29:29.103686Z",
          "shell.execute_reply": "2024-01-12T18:29:29.103508Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.103491Z"
        },
        "id": "p6R1HFVT_UYS",
        "outputId": "28a7c7ae-9d87-4953-b0c2-424fc3ca11b3",
        "trusted": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single train example: {'input_ids': [101, 2036, 1045, 2001, 1996, 2391, 2711, 2006, 2026, 2194, 1005, 1055, 6653, 2013, 1996, 1047, 2140, 1011, 1019, 2000, 24665, 1011, 1020, 2291, 1012, 2017, 2442, 1005, 2310, 2018, 2115, 2398, 2440, 1012, 2008, 1045, 2106, 1012, 2008, 1045, 2106, 1012, 2061, 2292, 1005, 1055, 2831, 1037, 2210, 2978, 2055, 2115, 5704, 1012, 2026, 5704, 1029, 2035, 2157, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\n",
            "Single validation example: {'input_ids': [101, 2852, 1012, 21500, 3917, 1010, 2045, 1005, 1055, 1037, 2835, 2058, 2182, 1012, 4067, 2017, 1010, 2852, 1012, 8109, 1010, 2021, 1045, 1005, 1049, 2383, 2026, 6265, 2012, 2023, 2795, 1010, 2182, 1999, 1996, 2690, 1012, 1045, 1005, 1049, 2383, 6265, 2157, 2182, 1010, 2007, 2026, 2204, 2767, 9558, 1010, 2065, 2002, 1005, 2222, 4133, 2007, 2033, 1012, 1045, 2097, 4133, 2007, 2017, 2852, 1012, 21500, 3917, 1012, 1061, 1005, 2113, 1010, 2057, 2147, 1999, 1037, 2688, 1997, 3019, 2381, 1010, 1998, 2664, 2045, 2003, 2242, 21242, 2055, 1996, 2126, 2057, 4521, 6265, 1012, 2085, 1010, 1045, 2298, 2105, 2023, 16673, 1010, 1998, 1061, 1005, 2113, 2054, 1045, 2156, 1010, 1045, 2156, 1011, 1045, 2156, 2407, 1012, 2407, 1010, 2090, 2111, 1999, 2317, 15695, 1998, 2111, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0]}\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "emotions = df['emotions'].explode().unique()\n",
        "num_labels = len(emotions)\n",
        "# Initialize the tokenizer\n",
        "model_card = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_card)\n",
        "\n",
        "# Initialize MultiLabelBinarizer for emotions\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit(train_data['emotions'])\n",
        "\n",
        "def preprocess_data(data, tokenizer, mlb):\n",
        "    # Tokenize the utterances\n",
        "    tokenized_inputs = tokenizer(\n",
        "        [\" \".join(utterance) for utterance in data['utterances']],\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=128, # we might wanna change this to max_len !\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Encode the emotions\n",
        "    encoded_labels = mlb.transform(data['emotions'])\n",
        "    # Convert labels to tensors\n",
        "    labels_tensor = torch.tensor(encoded_labels, dtype=torch.float)\n",
        "    # Ensure the encoded labels are the correct shape (this step is crucial to avoid the batch size mismatch)\n",
        "    if labels_tensor.shape[0] != tokenized_inputs['input_ids'].shape[0]:\n",
        "        raise ValueError(f\"Number of examples {tokenized_inputs['input_ids'].shape[0]} does not match number of labels {labels_tensor.shape[0]}\")\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': labels_tensor\n",
        "    })\n",
        "\n",
        "# Now call preprocess_data to process your train and validation data\n",
        "train_dataset = preprocess_data(train_data, tokenizer, mlb)\n",
        "val_dataset = preprocess_data(val_data, tokenizer, mlb)\n",
        "\n",
        "print(\"Single train example:\", train_dataset[0])\n",
        "print(\"Single validation example:\",val_dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset['input_ids'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4pFIyCSp9kd",
        "outputId": "fc32b7c4-1b6d-422a-ce69-468b8da47cb5"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[101,\n",
              "  2852,\n",
              "  1012,\n",
              "  21500,\n",
              "  3917,\n",
              "  1010,\n",
              "  2045,\n",
              "  1005,\n",
              "  1055,\n",
              "  1037,\n",
              "  2835,\n",
              "  2058,\n",
              "  2182,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  4067,\n",
              "  2017,\n",
              "  1010,\n",
              "  2852,\n",
              "  1012,\n",
              "  8109,\n",
              "  1010,\n",
              "  2021,\n",
              "  1045,\n",
              "  1005,\n",
              "  1049,\n",
              "  2383,\n",
              "  2026,\n",
              "  6265,\n",
              "  2012,\n",
              "  2023,\n",
              "  2795,\n",
              "  1010,\n",
              "  2182,\n",
              "  1999,\n",
              "  1996,\n",
              "  2690,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  1045,\n",
              "  1005,\n",
              "  1049,\n",
              "  2383,\n",
              "  6265,\n",
              "  2157,\n",
              "  2182,\n",
              "  1010,\n",
              "  2007,\n",
              "  2026,\n",
              "  2204,\n",
              "  2767,\n",
              "  9558,\n",
              "  1010,\n",
              "  2065,\n",
              "  2002,\n",
              "  1005,\n",
              "  2222,\n",
              "  4133,\n",
              "  2007,\n",
              "  2033,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  1045,\n",
              "  2097,\n",
              "  4133,\n",
              "  2007,\n",
              "  2017,\n",
              "  2852,\n",
              "  1012,\n",
              "  21500,\n",
              "  3917,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  1061,\n",
              "  1005,\n",
              "  2113,\n",
              "  1010,\n",
              "  2057,\n",
              "  2147,\n",
              "  1999,\n",
              "  1037,\n",
              "  2688,\n",
              "  1997,\n",
              "  3019,\n",
              "  2381,\n",
              "  1010,\n",
              "  1998,\n",
              "  2664,\n",
              "  2045,\n",
              "  2003,\n",
              "  2242,\n",
              "  21242,\n",
              "  2055,\n",
              "  1996,\n",
              "  2126,\n",
              "  2057,\n",
              "  4521,\n",
              "  6265,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  2085,\n",
              "  1010,\n",
              "  1045,\n",
              "  2298,\n",
              "  2105,\n",
              "  2023,\n",
              "  16673,\n",
              "  1010,\n",
              "  1998,\n",
              "  1061,\n",
              "  1005,\n",
              "  2113,\n",
              "  2054,\n",
              "  1045,\n",
              "  2156,\n",
              "  1010,\n",
              "  1045,\n",
              "  2156,\n",
              "  1011,\n",
              "  1045,\n",
              "  2156,\n",
              "  2407,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  2407,\n",
              "  1010,\n",
              "  2090,\n",
              "  2111,\n",
              "  1999,\n",
              "  2317,\n",
              "  15695,\n",
              "  1998,\n",
              "  2111,\n",
              "  1999,\n",
              "  2630,\n",
              "  28513,\n",
              "  1010,\n",
              "  1998,\n",
              "  1045,\n",
              "  3198,\n",
              "  2870,\n",
              "  1010,\n",
              "  1000,\n",
              "  2026,\n",
              "  2643,\n",
              "  2339,\n",
              "  1029,\n",
              "  999,\n",
              "  1000,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  2085,\n",
              "  1010,\n",
              "  1045,\n",
              "  2360,\n",
              "  2057,\n",
              "  8328,\n",
              "  2122,\n",
              "  1011,\n",
              "  2122,\n",
              "  15695,\n",
              "  2008,\n",
              "  3584,\n",
              "  2149,\n",
              "  1010,\n",
              "  1998,\n",
              "  2057,\n",
              "  2131,\n",
              "  2000,\n",
              "  2113,\n",
              "  1996,\n",
              "  2111,\n",
              "  7650,\n",
              "  1012,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  1045,\n",
              "  1005,\n",
              "  1049,\n",
              "  5811,\n",
              "  999,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0],\n",
              " [101,\n",
              "  0,\n",
              "  102,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0,\n",
              "  0]]"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.105207Z",
          "iopub.status.idle": "2024-01-12T18:29:29.105538Z",
          "shell.execute_reply": "2024-01-12T18:29:29.105392Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.105378Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "CVLkqwTGDn1P",
        "outputId": "f52cf683-4b7e-46bf-fc1b-b34e8a906182"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 00:26, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Sequence F1</th>\n",
              "      <th>Unrolled Sequence F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.578000</td>\n",
              "      <td>0.581855</td>\n",
              "      <td>0.475428</td>\n",
              "      <td>0.743736</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 00:26, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Sequence F1</th>\n",
              "      <th>Unrolled Sequence F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.491800</td>\n",
              "      <td>0.514826</td>\n",
              "      <td>0.512398</td>\n",
              "      <td>0.765638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5148258209228516, 'eval_sequence_f1': 0.5123978547771176, 'eval_unrolled_sequence_f1': 0.7656380316930776, 'eval_runtime': 2.7564, 'eval_samples_per_second': 145.117, 'eval_steps_per_second': 2.54, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy = \"epoch\"\n",
        ")\n",
        "\n",
        "# Initialize the model with the number of emotion labels\n",
        "num_labels = len(mlb.classes_)\n",
        "model = BertForSequenceClassification.from_pretrained(model_card, num_labels=num_labels)\n",
        "# Freeze the BERT embedding layer weights\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name: # Layer names not containing 'classifier' will be frozen\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "# Train the model\n",
        "trainer.train()\n",
        "# Train the model\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_prediction_info = trainer.predict(val_dataset)\n",
        "#metrics = compute_metrics(test_prediction_info)\n",
        "#print(test_prediction_info)\n",
        "test_prediction_info.predictions[0],test_prediction_info.label_ids[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "INbj8lLHhH-e",
        "outputId": "3b2499f1-5e3f-43b4-a4c8-d40c221cb3bf"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([ 2.447779  ,  2.1871235 ,  0.77851427, -1.3586662 , -0.32813776,\n",
              "         1.0689657 , -1.5161765 ,  0.18191618], dtype=float32),\n",
              " array([1., 1., 0., 1., 1., 1., 0., 0.], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-LN0VtFJ19_"
      },
      "source": [
        "## For two classifier heads, one common model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.106729Z",
          "iopub.status.idle": "2024-01-12T18:29:29.107060Z",
          "shell.execute_reply": "2024-01-12T18:29:29.106915Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.106900Z"
        },
        "id": "ePRUH-k7Yg57",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install datasets==2.13.2\n",
        "!pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.108963Z",
          "iopub.status.idle": "2024-01-12T18:29:29.109272Z",
          "shell.execute_reply": "2024-01-12T18:29:29.109134Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.109119Z"
        },
        "id": "bTfpfqKOJz-d",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from transformers import BertPreTrainedModel, BertModel\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.nn import BCEWithLogitsLoss, Linear\n",
        "import torch\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from datasets import Dataset\n",
        "\n",
        "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config, num_labels_emotions, num_labels_triggers):\n",
        "        super().__init__(config)\n",
        "        self.num_labels_emotions = num_labels_emotions\n",
        "        self.num_labels_triggers = num_labels_triggers\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        self.classifier_emotions = Linear(config.hidden_size, num_labels_emotions)\n",
        "        self.classifier_triggers = Linear(config.hidden_size, num_labels_triggers)\n",
        "\n",
        "        # You can initialize weights here if needed\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        labels_emotions=None,\n",
        "        labels_triggers=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        logits_emotions = self.classifier_emotions(pooled_output)\n",
        "        logits_triggers = self.classifier_triggers(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        # Calculate loss only if both labels are provided\n",
        "        if labels_emotions is not None and labels_triggers is not None:\n",
        "            loss_fct = BCEWithLogitsLoss()\n",
        "            loss_emotions = loss_fct(logits_emotions.view(-1, self.num_labels_emotions), labels_emotions.view(-1))\n",
        "            loss_triggers = loss_fct(logits_triggers.view(-1, self.num_labels_triggers), labels_triggers.view(-1))\n",
        "            loss = loss_emotions + loss_triggers\n",
        "\n",
        "        # Adjust the return statement to match what your compute_loss method expects\n",
        "        if not return_dict:\n",
        "            outputs = (logits_emotions, logits_triggers)\n",
        "            if loss is not None:\n",
        "                outputs = (loss,) + outputs\n",
        "        else:\n",
        "            outputs = {'loss': loss, 'logits_emotions': logits_emotions, 'logits_triggers': logits_triggers}\n",
        "\n",
        "        return outputs\n",
        "        \"\"\"\n",
        "        if not return_dict:\n",
        "            print(f\"Logits Emotions: {logits_emotions.shape}, Logits Triggers: {logits_triggers.shape}\")\n",
        "            output = (logits_emotions, logits_triggers) if loss is None else (loss, logits_emotions, logits_triggers)\n",
        "            return (loss, logits_emotions, logits_triggers) if loss is not None else (logits_emotions, logits_triggers)\n",
        "        else:\n",
        "            raise NotImplementedError(\"return_dict is set to True, but this model does not support it.\")\n",
        "            \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.110848Z",
          "iopub.status.idle": "2024-01-12T18:29:29.111189Z",
          "shell.execute_reply": "2024-01-12T18:29:29.111035Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.111019Z"
        },
        "id": "3hijFGmeMkAE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize MultiLabelBinarizers\n",
        "mlb_emotions = MultiLabelBinarizer()\n",
        "mlb_triggers = MultiLabelBinarizer()\n",
        "mlb_emotions.fit(train_data['emotions'])\n",
        "mlb_triggers.fit(train_data['triggers'])\n",
        "\n",
        "# Initialize the tokenizer\n",
        "model_card = 'bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_card)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.112750Z",
          "iopub.status.idle": "2024-01-12T18:29:29.113189Z",
          "shell.execute_reply": "2024-01-12T18:29:29.112978Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.112958Z"
        },
        "id": "zuvufmF0mieE",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data, tokenizer, mlb_emotions, mlb_triggers):\n",
        "    tokenized_inputs = tokenizer(\n",
        "      [\" \".join(utterance) for utterance in data['utterances']],\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      max_length=128,\n",
        "      return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Encode the emotions and triggers\n",
        "    encoded_labels_emotions = mlb_emotions.transform(data['emotions'])\n",
        "    encoded_labels_triggers = mlb_triggers.transform(data['triggers'])\n",
        "\n",
        "    # Convert labels to tensors and reshape\n",
        "    labels_emotions = torch.tensor(encoded_labels_emotions, dtype=torch.float).view(-1, mlb_emotions.classes_.size)\n",
        "    labels_triggers = torch.tensor(encoded_labels_triggers, dtype=torch.float).view(-1, mlb_triggers.classes_.size)\n",
        "\n",
        "    return Dataset.from_dict({\n",
        "        'input_ids': tokenized_inputs['input_ids'],\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels_emotions': labels_emotions,\n",
        "        'labels_triggers': labels_triggers\n",
        "    })\n",
        "\n",
        "# Then preprocess the training and validation data\n",
        "train_dataset = preprocess_data(train_data, tokenizer, mlb_emotions, mlb_triggers)\n",
        "val_dataset = preprocess_data(val_data, tokenizer, mlb_emotions, mlb_triggers)\n",
        "\n",
        "print(\"Single train example:\", train_dataset[0])\n",
        "print(\"Single validation example:\",val_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.114740Z",
          "iopub.status.idle": "2024-01-12T18:29:29.115104Z",
          "shell.execute_reply": "2024-01-12T18:29:29.114945Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.114928Z"
        },
        "id": "bRKm76SLUkZt",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Define your compute_metrics function to handle both sets of labels\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    logits_emotions = logits[0]\n",
        "    logits_triggers = logits[1]\n",
        "    labels_emotions = labels[0]\n",
        "    labels_triggers = labels[1]\n",
        "    # Sigmoid function to convert logits to probabilities\n",
        "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
        "    # Threshold to convert probabilities to binary predictions\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Convert logits to probabilities\n",
        "    probs_emotions = sigmoid(logits_emotions)\n",
        "    probs_triggers = sigmoid(logits_triggers)\n",
        "\n",
        "    # Convert probabilities to binary predictions\n",
        "    preds_emotions = (probs_emotions > threshold).astype(int)\n",
        "    preds_triggers = (probs_triggers > threshold).astype(int)\n",
        "\n",
        "    # Calculate the f1-score for emotions\n",
        "    f1_emotions = f1_score(labels_emotions, preds_emotions, average='macro')\n",
        "    # Calculate the f1-score for triggers\n",
        "    f1_triggers = f1_score(labels_triggers, preds_triggers, average='macro')\n",
        "\n",
        "    # To calculate sequence-level metrics, we might need to modify data processing\n",
        "\n",
        "    # to include sequence IDs, and then group by sequence ID before calculating metrics.\n",
        "    return {\n",
        "        'f1_emotions': f1_emotions,\n",
        "        'f1_triggers': f1_triggers,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.116605Z",
          "iopub.status.idle": "2024-01-12T18:29:29.116946Z",
          "shell.execute_reply": "2024-01-12T18:29:29.116806Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.116790Z"
        },
        "id": "w_IrhqCwnYw5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "#TODO make this compatible with model class by changing the way the absence of loss is handled, similar to CustomTrainer4\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Unpack inputs\n",
        "        labels_emotions = inputs.pop(\"labels_emotions\", None)\n",
        "        labels_triggers = inputs.pop(\"labels_triggers\", None)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # If loss is not included in outputs, calculate it using the logits\n",
        "        if len(outputs) == 2:\n",
        "            logits_emotions, logits_triggers = outputs\n",
        "            loss_fct = BCEWithLogitsLoss()\n",
        "            if labels_emotions is not None and labels_triggers is not None:\n",
        "                loss_emotions = loss_fct(logits_emotions.view(-1, self.model.num_labels_emotions), labels_emotions.view(-1, self.model.num_labels_emotions))\n",
        "                loss_triggers = loss_fct(logits_triggers.view(-1, self.model.num_labels_triggers), labels_triggers.view(-1, self.model.num_labels_triggers))\n",
        "                loss = loss_emotions + loss_triggers\n",
        "        elif len(outputs) == 3:\n",
        "            # Outputs include a loss\n",
        "            loss, logits_emotions, logits_triggers = outputs\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected number of outputs from model: {len(outputs)}\")\n",
        "\n",
        "        # If loss was not computed within the model, we compute it based on the logits and the labels\n",
        "        if loss is None and labels_emotions is not None and labels_triggers is not None:\n",
        "            loss_fct = BCEWithLogitsLoss()\n",
        "            loss_emotions = loss_fct(logits_emotions.view(-1, self.model.num_labels_emotions), labels_emotions.view(-1))\n",
        "            loss_triggers = loss_fct(logits_triggers.view(-1, self.model.num_labels_triggers), labels_triggers.view(-1))\n",
        "            loss = loss_emotions + loss_triggers\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.117887Z",
          "iopub.status.idle": "2024-01-12T18:29:29.118226Z",
          "shell.execute_reply": "2024-01-12T18:29:29.118081Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.118065Z"
        },
        "id": "4-kDAr-dj54R",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class CustomTrainer4(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Unpack inputs\n",
        "        labels_emotions = inputs.pop(\"labels_emotions\", None)\n",
        "        labels_triggers = inputs.pop(\"labels_triggers\", None)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # If loss is not included in outputs, calculate it using the logits\n",
        "        if 'loss' not in outputs:\n",
        "            logits_emotions = outputs['logits_emotions']\n",
        "            logits_triggers = outputs['logits_triggers']\n",
        "            loss_fct = BCEWithLogitsLoss()\n",
        "            if labels_emotions is not None:\n",
        "                loss_emotions = loss_fct(logits_emotions.view(-1, self.model.num_labels_emotions), labels_emotions.view(-1))\n",
        "            if labels_triggers is not None:\n",
        "                loss_triggers = loss_fct(logits_triggers.view(-1, self.model.num_labels_triggers), labels_triggers.view(-1))\n",
        "            loss = loss_emotions + loss_triggers\n",
        "            outputs['loss'] = loss\n",
        "        else:\n",
        "            # Loss is included in outputs\n",
        "            loss = outputs['loss']\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.119377Z",
          "iopub.status.idle": "2024-01-12T18:29:29.119738Z",
          "shell.execute_reply": "2024-01-12T18:29:29.119559Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.119543Z"
        },
        "id": "_tE4h1L9MzY0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Initialize model with the number of emotion and trigger labels\n",
        "num_labels_emotions = len(mlb_emotions.classes_)\n",
        "num_labels_triggers = len(mlb_triggers.classes_)\n",
        "model = BertForMultiLabelSequenceClassification.from_pretrained(\n",
        "    model_card,\n",
        "    num_labels_emotions=num_labels_emotions,\n",
        "    num_labels_triggers=num_labels_triggers\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.121255Z",
          "iopub.status.idle": "2024-01-12T18:29:29.121597Z",
          "shell.execute_reply": "2024-01-12T18:29:29.121442Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.121426Z"
        },
        "id": "KzER6NeRe3s0",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.123103Z",
          "iopub.status.idle": "2024-01-12T18:29:29.123443Z",
          "shell.execute_reply": "2024-01-12T18:29:29.123288Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.123272Z"
        },
        "id": "zMKcMdBeQnHx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Assuming train_dataset is a Hugging Face dataset with the correct columns\n",
        "example = train_dataset[0]\n",
        "\n",
        "# Now, before passing tensors to the model, ensure they are also on the same device\n",
        "example_tensors = {k: torch.tensor([v]).to(device) for k, v in example.items()}\n",
        "\n",
        "# Call the model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**example_tensors)\n",
        "    print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-12T18:29:29.124452Z",
          "iopub.status.idle": "2024-01-12T18:29:29.124807Z",
          "shell.execute_reply": "2024-01-12T18:29:29.124624Z",
          "shell.execute_reply.started": "2024-01-12T18:29:29.124609Z"
        },
        "id": "BcckkL0ILyll",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy = \"epoch\"\n",
        "    report_to = \"\"\n",
        ")\n",
        "\n",
        "# # Initialize the model with the number of emotion labels\n",
        "# num_labels = len(mlb.classes_)\n",
        "# model = BertForSequenceClassification.from_pretrained(model_card,\n",
        "#                                                       num_labels=num_labels,\n",
        "#                                                        )\n",
        "\n",
        "# Freeze the BERT embedding layer weights\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name: # Layer names not containing 'classifier' will be frozen\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics = compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "results = trainer.evaluate()\n",
        "\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dB6fTqFKCM_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 4296178,
          "sourceId": 7390484,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30636,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}