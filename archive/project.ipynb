{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7435709,"sourceType":"datasetVersion","datasetId":4327472}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- We loose structure of dialogue using the tokenizer in preprocess_data\n\n- Emotions encoded using MultiLabelBinarizer doesn't tell us\n  anymore how many times a single emotion is present in the dialogue\n  and where it is","metadata":{"id":"RgHMqcneDn1J"}},{"cell_type":"code","source":"#!pip install torch==1.13.0+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n!pip install transformers==4.30.0\n!pip install datasets==2.13.2\n!pip install accelerate -U\n!pip install evaluate","metadata":{"id":"vc-YdlGVyh9_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e624115d-acee-4553-8837-fb9cf4b04870","execution":{"iopub.status.busy":"2024-01-19T14:52:31.572284Z","iopub.execute_input":"2024-01-19T14:52:31.572710Z","iopub.status.idle":"2024-01-19T14:53:37.461936Z","shell.execute_reply.started":"2024-01-19T14:52:31.572674Z","shell.execute_reply":"2024-01-19T14:53:37.460952Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting transformers==4.30.0\n  Obtaining dependency information for transformers==4.30.0 from https://files.pythonhosted.org/packages/e2/72/1af3d38e98fdcceb3876de4567ac395a66c26976e259fe2d46266e052d61/transformers-4.30.0-py3-none-any.whl.metadata\n  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2.31.0)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.30.0) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2023.11.17)\nDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.0\n    Uninstalling tokenizers-0.15.0:\n      Successfully uninstalled tokenizers-0.15.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.36.2\n    Uninstalling transformers-4.36.2:\n      Successfully uninstalled transformers-4.36.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires transformers>=4.33.1, but you have transformers 4.30.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.30.0\nCollecting datasets==2.13.2\n  Obtaining dependency information for datasets==2.13.2 from https://files.pythonhosted.org/packages/d3/95/ef83542e7a8e2bfc4432ee2cd8a6b52eb30fb1e605871e8871e94ce65fb1/datasets-2.13.2-py3-none-any.whl.metadata\n  Downloading datasets-2.13.2-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (11.0.0)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.13.2)\n  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.13.2) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.13.2) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.2) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets==2.13.2) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.13.2) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.2) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.2) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.13.2) (2023.11.17)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.13.2)\n  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.2) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.2) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.13.2) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.13.2) (1.16.0)\nDownloading datasets-2.13.2-py3-none-any.whl (512 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.7/512.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: dill, multiprocess, datasets\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.7\n    Uninstalling dill-0.3.7:\n      Successfully uninstalled dill-0.3.7\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.15\n    Uninstalling multiprocess-0.70.15:\n      Successfully uninstalled multiprocess-0.70.15\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\npathos 0.3.1 requires dill>=0.3.7, but you have dill 0.3.6 which is incompatible.\npathos 0.3.1 requires multiprocess>=0.70.15, but you have multiprocess 0.70.14 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.13.2 dill-0.3.6 multiprocess-0.70.14\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.25.0)\nCollecting accelerate\n  Obtaining dependency information for accelerate from https://files.pythonhosted.org/packages/a6/b9/44623bdb05595481107153182e7f4b9f2ef9d3b674938ad13842054dcbd8/accelerate-0.26.1-py3-none-any.whl.metadata\n  Downloading accelerate-0.26.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.12.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.25.0\n    Uninstalling accelerate-0.25.0:\n      Successfully uninstalled accelerate-0.25.0\nSuccessfully installed accelerate-0.26.1\nCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.13.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m304.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n","output_type":"stream"}]},{"cell_type":"code","source":"# system packages\nfrom pathlib import Path\nimport shutil\nimport urllib\nimport tarfile\nimport sys\nimport os\n# data and numerical management packages\nimport pandas as pd\nimport random\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n# useful during debugging (progress bars)\nfrom tqdm import tqdm\nfrom transformers import set_seed\n\nseed = 852\n\nrandom.seed(seed)\nnp.random.seed(seed)\nset_seed(seed)","metadata":{"id":"pD7fWLEPyh-B","execution":{"iopub.status.busy":"2024-01-19T14:54:24.175501Z","iopub.execute_input":"2024-01-19T14:54:24.175879Z","iopub.status.idle":"2024-01-19T14:54:47.215342Z","shell.execute_reply.started":"2024-01-19T14:54:24.175847Z","shell.execute_reply":"2024-01-19T14:54:47.214536Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"dataset_folder = Path.cwd().joinpath(\"MELD_train_efr.json\")\n#dataset_path = dataset_folder.joinpath('/MELD_train_efr.json')\ndataset_folder = \"/kaggle/input/plaplapla/MELD_train_efr.json\"\ndf = pd.read_json(dataset_folder)\n#df['triggers'] = df['triggers'].fillna(value=0, inplace=False)#.replace('None', 0.0)","metadata":{"id":"6Vnl6q1Qyh-C","execution":{"iopub.status.busy":"2024-01-19T14:54:52.271253Z","iopub.execute_input":"2024-01-19T14:54:52.271898Z","iopub.status.idle":"2024-01-19T14:54:52.422899Z","shell.execute_reply.started":"2024-01-19T14:54:52.271866Z","shell.execute_reply":"2024-01-19T14:54:52.421043Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"id":"oq9_ohFH3yeX","colab":{"base_uri":"https://localhost:8080/","height":597},"outputId":"e2f2accf-19b7-440b-d0b2-0c8ba04893f7","execution":{"iopub.status.busy":"2024-01-19T14:54:53.822422Z","iopub.execute_input":"2024-01-19T14:54:53.823318Z","iopub.status.idle":"2024-01-19T14:54:53.861339Z","shell.execute_reply.started":"2024-01-19T14:54:53.823286Z","shell.execute_reply":"2024-01-19T14:54:53.860540Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"             episode                                           speakers  \\\n0        utterance_0  [Chandler, The Interviewer, Chandler, The Inte...   \n1        utterance_1  [Chandler, The Interviewer, Chandler, The Inte...   \n2        utterance_2  [Chandler, The Interviewer, Chandler, The Inte...   \n3        utterance_3  [Chandler, The Interviewer, Chandler, The Inte...   \n4        utterance_4                       [Joey, Rachel, Joey, Rachel]   \n...              ...                                                ...   \n3995  utterance_3995  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n3996  utterance_3996  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n3997  utterance_3997  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n3998  utterance_3998  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n3999  utterance_3999  [Chandler, All, Monica, Chandler, Ross, Chandl...   \n\n                                               emotions  \\\n0        [neutral, neutral, neutral, neutral, surprise]   \n1     [neutral, neutral, neutral, neutral, surprise,...   \n2     [neutral, neutral, neutral, neutral, surprise,...   \n3     [neutral, neutral, neutral, neutral, surprise,...   \n4                   [surprise, sadness, surprise, fear]   \n...                                                 ...   \n3995  [neutral, joy, neutral, neutral, surprise, dis...   \n3996  [neutral, joy, neutral, neutral, surprise, dis...   \n3997  [neutral, joy, neutral, neutral, surprise, dis...   \n3998  [neutral, joy, neutral, neutral, surprise, dis...   \n3999  [neutral, joy, neutral, neutral, surprise, dis...   \n\n                                             utterances  \\\n0     [also I was the point person on my company's t...   \n1     [also I was the point person on my company's t...   \n2     [also I was the point person on my company's t...   \n3     [also I was the point person on my company's t...   \n4     [But then who? The waitress I went out with la...   \n...                                                 ...   \n3995  [Hey., Hey!, So how was Joan?, I broke up with...   \n3996  [Hey., Hey!, So how was Joan?, I broke up with...   \n3997  [Hey., Hey!, So how was Joan?, I broke up with...   \n3998  [Hey., Hey!, So how was Joan?, I broke up with...   \n3999  [Hey., Hey!, So how was Joan?, I broke up with...   \n\n                                               triggers  \n0                             [0.0, 0.0, 0.0, 1.0, 0.0]  \n1                   [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]  \n2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n3     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n4                                  [0.0, 0.0, 1.0, 0.0]  \n...                                                 ...  \n3995  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n3996  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n3997  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n3998  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n3999  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n\n[4000 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>episode</th>\n      <th>speakers</th>\n      <th>emotions</th>\n      <th>utterances</th>\n      <th>triggers</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>utterance_0</td>\n      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n      <td>[neutral, neutral, neutral, neutral, surprise]</td>\n      <td>[also I was the point person on my company's t...</td>\n      <td>[0.0, 0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>utterance_1</td>\n      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n      <td>[also I was the point person on my company's t...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>utterance_2</td>\n      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n      <td>[also I was the point person on my company's t...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>utterance_3</td>\n      <td>[Chandler, The Interviewer, Chandler, The Inte...</td>\n      <td>[neutral, neutral, neutral, neutral, surprise,...</td>\n      <td>[also I was the point person on my company's t...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>utterance_4</td>\n      <td>[Joey, Rachel, Joey, Rachel]</td>\n      <td>[surprise, sadness, surprise, fear]</td>\n      <td>[But then who? The waitress I went out with la...</td>\n      <td>[0.0, 0.0, 1.0, 0.0]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3995</th>\n      <td>utterance_3995</td>\n      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3996</th>\n      <td>utterance_3996</td>\n      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3997</th>\n      <td>utterance_3997</td>\n      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3998</th>\n      <td>utterance_3998</td>\n      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n    <tr>\n      <th>3999</th>\n      <td>utterance_3999</td>\n      <td>[Chandler, All, Monica, Chandler, Ross, Chandl...</td>\n      <td>[neutral, joy, neutral, neutral, surprise, dis...</td>\n      <td>[Hey., Hey!, So how was Joan?, I broke up with...</td>\n      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>4000 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"triggers = df['triggers']\nfor row in range(triggers.shape[0]):\n    for trigger in range(len(triggers[row])):\n        if triggers[row][trigger] == None:\n            triggers[row][trigger] = 0.0\n\ndf['triggers'] = triggers","metadata":{"id":"PK4MQkLvyh-C","execution":{"iopub.status.busy":"2024-01-19T14:55:01.577721Z","iopub.execute_input":"2024-01-19T14:55:01.578604Z","iopub.status.idle":"2024-01-19T14:55:01.766066Z","shell.execute_reply.started":"2024-01-19T14:55:01.578558Z","shell.execute_reply":"2024-01-19T14:55:01.765107Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"emotions = df['emotions'].explode().unique()\nemotions","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFfykazN2xDw","outputId":"862c56ba-fff9-4500-a761-b5a13edf0d91","execution":{"iopub.status.busy":"2024-01-19T14:55:01.767909Z","iopub.execute_input":"2024-01-19T14:55:01.768190Z","iopub.status.idle":"2024-01-19T14:55:01.786308Z","shell.execute_reply.started":"2024-01-19T14:55:01.768166Z","shell.execute_reply":"2024-01-19T14:55:01.785534Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array(['neutral', 'surprise', 'fear', 'sadness', 'joy', 'disgust',\n       'anger'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"triggers = df['triggers'].explode().unique()\ntriggers","metadata":{"id":"p10v1luzDn1M","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fcbbdd4f-8320-40e8-cbbb-18b424217a6d","execution":{"iopub.status.busy":"2024-01-19T14:55:01.787508Z","iopub.execute_input":"2024-01-19T14:55:01.787866Z","iopub.status.idle":"2024-01-19T14:55:01.801047Z","shell.execute_reply.started":"2024-01-19T14:55:01.787832Z","shell.execute_reply":"2024-01-19T14:55:01.800146Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"array([0.0, 1.0], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"dialogues = df['utterances']\n#print(sentences)\nmax_len_dialogue = 0\nindex = 0\nfor idx, dialogue in enumerate(dialogues):\n  if len(dialogue) > max_len_dialogue:\n    max_len_dialogue = len(dialogue)\n    index = idx\nmax_len_dialogue,index","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q39q6ZPh-tCy","outputId":"f6b48818-5079-4e6b-cdcb-01da074fdf25","execution":{"iopub.status.busy":"2024-01-19T14:55:01.802004Z","iopub.execute_input":"2024-01-19T14:55:01.802575Z","iopub.status.idle":"2024-01-19T14:55:01.815797Z","shell.execute_reply.started":"2024-01-19T14:55:01.802544Z","shell.execute_reply":"2024-01-19T14:55:01.814877Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(24, 219)"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nsorted_emotions = sorted(emotions)  #sort the array because Binarizer will automatically do that for one hot encoding\nlabel_binarizer = LabelBinarizer()\nlabel_binarizer.fit(sorted_emotions)\n\ndialogues = df['emotions']\none_hot_emotions = []\nfor dialogue_emotion in dialogues:\n  dialogue_emotions_list = []\n  for emotion in dialogue_emotion:\n    encoded_emotion=label_binarizer.transform([emotion])\n    dialogue_emotions_list.append(np.ravel(encoded_emotion).tolist())\n  one_hot_emotions.append(dialogue_emotions_list)","metadata":{"id":"H6K5IJ0-NLlL","execution":{"iopub.status.busy":"2024-01-19T14:55:01.818813Z","iopub.execute_input":"2024-01-19T14:55:01.819198Z","iopub.status.idle":"2024-01-19T14:55:15.447863Z","shell.execute_reply.started":"2024-01-19T14:55:01.819174Z","shell.execute_reply":"2024-01-19T14:55:15.447026Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df['emotions'] = one_hot_emotions","metadata":{"id":"fNaWF4KIgIys","execution":{"iopub.status.busy":"2024-01-19T14:55:15.449068Z","iopub.execute_input":"2024-01-19T14:55:15.449401Z","iopub.status.idle":"2024-01-19T14:55:15.455432Z","shell.execute_reply.started":"2024-01-19T14:55:15.449371Z","shell.execute_reply":"2024-01-19T14:55:15.454554Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_data, temp_data = train_test_split(df, train_size=0.8, shuffle=False)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, shuffle=False)\nval_data.reset_index(drop=True, inplace=True)\ntest_data.reset_index(drop=True, inplace=True)","metadata":{"id":"ALZyV8V-kk5k","execution":{"iopub.status.busy":"2024-01-19T15:37:31.269362Z","iopub.execute_input":"2024-01-19T15:37:31.269811Z","iopub.status.idle":"2024-01-19T15:37:31.280717Z","shell.execute_reply.started":"2024-01-19T15:37:31.269776Z","shell.execute_reply":"2024-01-19T15:37:31.279818Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, Trainer, TrainingArguments\n\nclass CustomDataset(Dataset):\n    def __init__(self, dialogues, emotions, triggers, tokenizer, max_length=10):\n        self.dialogues = dialogues\n        self.emotions = emotions\n        self.triggers = triggers\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.dialogues)\n\n    def __getitem__(self, idx):\n        dialogue = self.dialogues[idx]\n        emotion = self.emotions[idx]\n        trigger = self.triggers[idx]\n\n        input_ids_list = []\n        attention_mask_list = []\n\n        for utterance in dialogue:\n          tokenized_utterance = self.tokenizer(utterance, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')\n          # Extract relevant information\n          #input_ids = torch.stack([inputs['input_ids'].squeeze() for inputs in tokenized_dialogue])\n          input_ids_list.extend(tokenized_utterance['input_ids'])\n          attention_mask_list.extend(tokenized_utterance['attention_mask'])\n\n        emotion_labels = torch.tensor(emotion, dtype=torch.float32)\n        trigger_label = torch.tensor(trigger, dtype=torch.long)\n        #print('input',torch.stack(input_ids_list).shape)\n        #print('attention',torch.stack(attention_mask_list).shape)\n        #print('emotion',emotion_labels.shape)\n        #print('trigeeer',trigger_label.shape)\n        return {\n            'input_ids': torch.stack(input_ids_list),\n            'attention_mask': torch.stack(attention_mask_list),\n            'emotion_labels': emotion_labels,\n            'trigger_label': trigger_label\n        }","metadata":{"id":"HiiN1Qs9k0DM","execution":{"iopub.status.busy":"2024-01-19T14:55:16.624483Z","iopub.execute_input":"2024-01-19T14:55:16.625040Z","iopub.status.idle":"2024-01-19T14:55:16.647212Z","shell.execute_reply.started":"2024-01-19T14:55:16.625013Z","shell.execute_reply":"2024-01-19T14:55:16.646482Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class CustomBERTModel(torch.nn.Module):\n    def __init__(self, freeze_embeddings=True):\n        super(CustomBERTModel, self).__init__()\n        # Replace this with your custom BERT model architecture for multihead classification\n        self.bert = BertModel.from_pretrained('bert-base-uncased')#BertForSequenceClassification.from_pretrained\n        #LSTM\n        if freeze_embeddings:\n            for name,param in self.bert.named_parameters():\n                if 'embeddings' in name:\n                    param.requires_grad = False\n\n        self.emotion_head = torch.nn.Linear(self.bert.config.hidden_size, len(emotions))\n        self.trigger_head = torch.nn.Linear(self.bert.config.hidden_size, len(triggers))\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n        pooled_output = outputs['pooler_output']\n\n        # Emotion head\n        emotion_logits = self.emotion_head(pooled_output)\n\n        # Trigger head\n        trigger_logits = self.trigger_head(pooled_output)\n        return emotion_logits, trigger_logits\n        #return emotion_outputs, trigger_outputs\n        #return torch.stack(emotion_outputs), torch.stack(trigger_outputs)","metadata":{"id":"MfLGTU4Mkw3V","execution":{"iopub.status.busy":"2024-01-19T16:59:48.636076Z","iopub.execute_input":"2024-01-19T16:59:48.636728Z","iopub.status.idle":"2024-01-19T16:59:48.644739Z","shell.execute_reply.started":"2024-01-19T16:59:48.636695Z","shell.execute_reply":"2024-01-19T16:59:48.643907Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"# Assuming train_dialogues, train_emotions, train_triggers, test_dialogues, test_emotions, test_triggers are defined\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\ntrain_dataset = CustomDataset(train_data['utterances'], train_data['emotions'], train_data['triggers'], tokenizer)\ntest_dataset = CustomDataset(val_data['utterances'], val_data['emotions'], val_data['triggers'], tokenizer)\n\nfreezed_embeddings = True\ncustom_Bert_Model = CustomBERTModel(freezed_embeddings)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncustom_Bert_Model = custom_Bert_Model.to(device)\noptimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=5e-5)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUR8qBNelMMl","outputId":"a720cc29-6cbe-408d-90b7-cb72b5de36e5","execution":{"iopub.status.busy":"2024-01-19T16:59:52.185789Z","iopub.execute_input":"2024-01-19T16:59:52.186155Z","iopub.status.idle":"2024-01-19T16:59:53.312702Z","shell.execute_reply.started":"2024-01-19T16:59:52.186126Z","shell.execute_reply":"2024-01-19T16:59:53.311901Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"num_epochs = 5\nbatch_size = 1\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n    \nfor epoch in range(num_epochs):\n    custom_Bert_Model.train()\n    total_loss = 0.0\n\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch + 1}', leave=False):\n        input_ids = batch['input_ids'].squeeze().to(device)\n        attention_mask = batch['attention_mask'].squeeze().to(device)\n        emotion_labels = batch['emotion_labels'].squeeze().to(device)\n        trigger_label = batch['trigger_label'].squeeze().to(device)\n        optimizer.zero_grad()\n        \n        emotion_logits, trigger_logits = custom_Bert_Model(input_ids, attention_mask)\n\n        # Assuming you have defined loss functions for emotion and trigger\n        emotion_loss = compute_metrics((emotion_logits, emotion_labels))\n        #trigger_loss = your_trigger_loss_function(trigger_logits, trigger_label)\n\n        loss = emotion_loss #+ trigger_loss\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    average_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}, Average Loss: {average_loss}\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"RiM7PzNfsHBm","outputId":"2ccd0efb-ffc6-4f71-c0f9-ee885f42745c","execution":{"iopub.status.busy":"2024-01-19T16:59:59.152258Z","iopub.execute_input":"2024-01-19T16:59:59.152636Z","iopub.status.idle":"2024-01-19T17:06:14.724472Z","shell.execute_reply.started":"2024-01-19T16:59:59.152604Z","shell.execute_reply":"2024-01-19T17:06:14.723552Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Average Loss: 0.03521169826708501\n","output_type":"stream"},{"name":"stderr","text":"                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2, Average Loss: 0.03519867061200785\n","output_type":"stream"},{"name":"stderr","text":"                                                            ","output_type":"stream"},{"name":"stdout","text":"Epoch 5, Average Loss: 0.0352083815788501\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"code","source":"neutral = (1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\nsurprise =(0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0)\nfear = (0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0)\nsadness = (0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0)\njoy = (0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0)\ndisgust = (0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0)\nanger = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0)\nmisclassified = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\n\nemotions_dictionary = {neutral : 'neutral',\n                       surprise : 'surprise',\n                       fear : 'fear',\n                       sadness : 'sadness',\n                       joy : 'joy',\n                       disgust : 'disgust',\n                       anger : 'anger',\n                       misclassified : 'misclassified'\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-19T17:06:14.726126Z","iopub.execute_input":"2024-01-19T17:06:14.726414Z","iopub.status.idle":"2024-01-19T17:06:14.734036Z","shell.execute_reply.started":"2024-01-19T17:06:14.726390Z","shell.execute_reply":"2024-01-19T17:06:14.733046Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nfrom sklearn.metrics import f1_score\n\ndef compute_sequence_f1(predictions, labels):\n    # predictions and labels should be lists of tensors for each dialogue\n    emotion_f1_scores = []\n    trigger_f1_scores = []\n    for emotion_pred, trigger_pred, emotion_lab, trigger_lab in zip(predictions[0], predictions[1], labels[0], labels[1]):\n        emotion_predicted_classes = torch.argmax(emotion_pred, dim=1)\n        trigger_predicted_classes = torch.argmax(trigger_pred, dim=1)\n        emotion_true_classes = torch.argmax(emotion_lab, dim=1)\n        trigger_true_classes = trigger_lab\n        emotion_f1 = f1_score(emotion_true_classes.cpu().numpy(), emotion_predicted_classes.cpu().numpy(), average='macro')\n        trigger_f1 = f1_score(trigger_true_classes.cpu().numpy(), trigger_predicted_classes.cpu().numpy(), average='macro')\n        emotion_f1_scores.append(emotion_f1)\n        trigger_f1_scores.append(trigger_f1)\n    average_emotion_f1 = torch.tensor(emotion_f1_scores, dtype=torch.float32).mean()\n    average_trigger_f1 = torch.tensor(trigger_f1_scores, dtype=torch.float32).mean()\n    return average_emotion_f1, average_trigger_f1\n\ndef compute_unrolled_sequence_f1(predictions, labels):\n    # Flatten all utterances and compute the F1 score\n    all_emotion_predicted_classes = torch.argmax(torch.cat(predictions[0], dim=0), dim=1)\n    all_trigger_predicted_classes = torch.argmax(torch.cat(predictions[1], dim=0), dim=1)\n    all_emotion_true_classes = torch.argmax(torch.cat(labels[0], dim=0), dim=1)\n    all_trigger_true_classes = torch.cat(labels[1], dim=0)\n    unrolled_emotion_f1 = f1_score(all_emotion_true_classes.cpu().numpy(), all_emotion_predicted_classes.cpu().numpy(), average='macro')\n    unrolled_trigger_f1 = f1_score(all_trigger_true_classes.cpu().numpy(), all_trigger_predicted_classes.cpu().numpy(), average='macro')\n    unrolled_emotion_f1_tensor = torch.tensor(unrolled_emotion_f1, dtype=torch.float32)\n    unrolled_trigger_f1_tensor = torch.tensor(unrolled_trigger_f1, dtype=torch.float32)\n    return unrolled_emotion_f1_tensor, unrolled_trigger_f1_tensor\n\n# Usage in the eval loop\nsequence_f1_scores_emotion = []\nsequence_f1_scores_trigger = []\nunrolled_predictions_emotion = []\nunrolled_predictions_trigger = []\nunrolled_labels_emotion = []\nunrolled_labels_trigger = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Evaluation', leave=False):\n        input_ids = batch['input_ids'].squeeze().to(device)\n        attention_mask = batch['attention_mask'].squeeze().to(device)\n        emotion_labels = batch['emotion_labels'].squeeze().to(device)\n        trigger_label = batch['trigger_label'].squeeze().to(device)\n\n        emotion_logits, trigger_logits = custom_Bert_Model(input_ids, attention_mask)\n\n        # Store predictions and labels for later unrolled F1 computation\n        unrolled_predictions_emotion.append(emotion_logits)\n        unrolled_labels_emotion.append(emotion_labels)\n        unrolled_predictions_trigger.append(trigger_logits)\n        unrolled_labels_trigger.append(trigger_label)\n        \n        # Convert logits to probabilities and then to class predictions\n        predicted_classes = torch.argmax(emotion_logits, dim=1)\n        true_classes = torch.argmax(emotion_labels, dim=1)\n\n        # Compute F1 for the current sequence (dialogue)\n        sequence_f1 = f1_score(true_classes.cpu().numpy(), predicted_classes.cpu().numpy(), average='macro')\n        sequence_f1_scores.append(sequence_f1)\n\n# Compute the average Sequence F1 for emotions and triggers\naverage_sequence_f1_emotion, average_sequence_f1_trigger = compute_sequence_f1(\n    [unrolled_predictions_emotion, unrolled_predictions_trigger], \n    [unrolled_labels_emotion, unrolled_labels_trigger]\n)\n\n# Compute the Unrolled Sequence F1 for emotions and triggers\nunrolled_sequence_f1_emotion, unrolled_sequence_f1_trigger = compute_unrolled_sequence_f1(\n    [unrolled_predictions_emotion, unrolled_predictions_trigger], \n    [unrolled_labels_emotion, unrolled_labels_trigger]\n)\n\n# Print the F1 scores for emotions and triggers\nprint(f\"Average Sequence F1 (Emotion): {average_sequence_f1_emotion}\")\nprint(f\"Average Sequence F1 (Trigger): {average_sequence_f1_trigger}\")\nprint(f\"Unrolled Sequence F1 (Emotion): {unrolled_sequence_f1_emotion.item()}\")\nprint(f\"Unrolled Sequence F1 (Trigger): {unrolled_sequence_f1_trigger.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-19T17:06:23.721431Z","iopub.execute_input":"2024-01-19T17:06:23.721829Z","iopub.status.idle":"2024-01-19T17:06:32.580790Z","shell.execute_reply.started":"2024-01-19T17:06:23.721805Z","shell.execute_reply":"2024-01-19T17:06:32.579851Z"},"trusted":true},"execution_count":113,"outputs":[{"name":"stderr","text":"                                                             \r","output_type":"stream"},{"name":"stdout","text":"Average Sequence F1 (Emotion): 0.04618513211607933\nAverage Sequence F1 (Trigger): 0.20829695463180542\nUnrolled Sequence F1 (Emotion): 0.02938121184706688\nUnrolled Sequence F1 (Trigger): 0.1943325698375702\n","output_type":"stream"}]}]}
